---
title: "gym_members"
author: "Nuria Oviedo"
date: "2025-02-06"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Introducción

El conjunto de datos contiene un análisis de patrines de actividad física y rendimiento en distintos niveles de experiencia en gimnasios.

Estos datos se han extraido de [kaggle](https://www.kaggle.com/datasets/valakhorasani/gym-members-exercise-dataset). Segun la propia descripción de los datos,este conjunto de datos nos proporciona una descripción detallada de las turinad de ejercicio, los atributos físicos y las métricas de aptitud física de los miembros de gimnasios. En temas de salud es muy importante llevar un monitoreo de las cosas que hay que hacer para estar bien físicamente. El objetivo de este sistema es poder buscar patrones y evidencias de lo que tiene que hacer una persona para estar bien fisica y mentalmente. Este conjunto de dato nos da información sobre datos demográficos y niveles de experiencia, lo que permite un análisis integral de los patrones de aptitud física, la progresión de los atletas y las tendencias de salud. Además de esto no 

Según la descripción oficial de los datos, las variables que conforman el conjunto de datos son:

* Edad: Edad del miembro del gimnasio.
* Género: Género del miembro del gimnasio (masculino o femenino).
* Peso (kg): Peso del miembro en kilogramos.
* Altura (m): Altura del miembro en metros.
* Max_BPM: Frecuencia cardíaca máxima (pulsaciones por minuto) durante las sesiones de entrenamiento.
* Avg_BPM: Frecuencia cardíaca promedio durante las sesiones de entrenamiento.
* Resting_BPM: Frecuencia cardíaca en reposo antes del entrenamiento.
* Session_Duration (horas): Duración de cada sesión de entrenamiento en horas.
* Calorías_Quemadas: Total de calorías quemadas durante cada sesión.
* Workout_Type: Tipo de entrenamiento realizado (por ejemplo, Cardio, Fuerza, Yoga, HIIT).
* Fat_Percentage: Porcentaje de grasa corporal del miembro.
* Consumo_de_agua (litros): Ingesta diaria de agua durante los entrenamientos.
* Frecuencia_de_entrenamiento (días/semana): Número de sesiones de entrenamiento por semana.
* Nivel_Experiencia: Nivel de experiencia, desde principiante (1) hasta experto (3).
* IMC: Índice de Masa Corporal, calculado a partir de la altura y el peso.

Dado que es un problema de regresión, la variable objetivo (Cantidad de calorias quemadas durante la sesión de entrenamiento) es continua.


# Cargar paquetes necesarios

```{r}

library(ggplot2)
library(car) # pruebas de diagnóstico
library(lmtest) # prueba de homocedasticidad
library(dplyr) # manipulación de datos
library(GGally) # visualización avanzada
library(HistData) # datos históricos de Galton
library(gridExtra) # organizar múltiples gráficos
```


# Data undestandig

## Cargamos los datos.
```{r}
datos <- read.csv("gym_members_exercise_tracking.csv")
```

## Tamaño del dataset.
```{r}
size <- dim(datos)
```

En estos datos podemos ver que tenemos un total de `r size[1]` observaciones y `r size [2]` variables en este dataset.

## Variables.
```{r}
str(datos)
```

Al ver esto podemos ver que estos datos son en su mayoría de tipo numérico o entero, sin embargo, tenemos dos variables de tipo cadena de caracteres (`char`), `Gender` y `Workout_Type`.

## Valores faltantes.
```{r}
colSums(is.na(datos))
```

Vemos que no tenemos ningun datos faltante o missing entre nuestros datos.

## Primer vistazo a los datos.
```{r}
head(datos, 5)
```

# EDA

Antes de nada vamos a ver información de nuestra variable objetivo importante
```{r}
summary(datos$Calories_Burned)
```
Conesto nos da una idea de la cantidad de calorias que las personas pueden quemar, vemos como la media es casi 900 kcalorias y vemos tambien que el mínimo es de unas 300 kcal y el máximo es de 1700 kcal. 

Vamos a ver la forma de nuestra variable objetivo gráficamente.
```{r}
datos |> ggplot(mapping = aes(x = Calories_Burned)) +
  geom_histogram()
```

Viendo este histograma se puede ver como las calorias quemadas se acerca a una distribución normal, viendo los datos de antes, se ve que este histograma si que tiene sentido. 

# Modelización estadística.
Nuestro problema para analizar es ver si hay alguna relación positiva entre las calorias que quema una persona por duración del entrenamineto, con esto buscamos como objetivo determinar si cuanto más tiempo una persona entrene, más kcalorias quema durante el entrenamiento. 

Las variables que se involucran seria el tiempo de duración de las sesiones de entrenamiento (variable explicativa) y la cantidad de calorias que quema una persona (variable respuesta).

Miramos los datos de la variable explicativa

```{r}
summary(datos$Session_Duration..hours.)
```

Podemos ver que las personas entrenan una media de 1.256 horas, lo que hace 1 hora y 15 minutos, además podemos ver que dentro de los datos registrados, el mínimo de horas que se ha almacenado es de 0.5 horas y el máximo es de 2 horas.

Observamos los datos de esas dos variables.

```{r}
problema <- datos |> select(Session_Duration..hours., Calories_Burned)
head(problema, 5)
```

Viendo esta cabecera, se puede ver a simple vista que si que puede haber una realción entre la duración de el entrenamiento con las calorias que se queman, para verlo más claro, representamos ambas variables graficamente.

```{r}
datos |>
  ggplot(mapping = aes(x = Calories_Burned, y = Session_Duration..hours.)) +
  geom_point()
```

Dado que tenemos una relación positiva entre estas dos variables, voy a proponer un modelso para relacionar estas dos variables., buscamos mediante el método de mínimos cuadrados hallar los valores de $\beta_0$ y $\beta_1$ para poder llegar a un modelo con la forma: $\text{Calorias} = \beta_0 + \beta_1 \text{(Tiempo de entrenamient0)} + \epsilon$.

```{r}
# Creamos el modelo lineal y lo analizamos
model <- lm(Calories_Burned ~ Session_Duration..hours., data = datos)
summary(model)
```

Ahora vamos a ver el modelo gráficamente.

```{r}
datos |> 
  ggplot(mapping = aes(x = Calories_Burned, y = Session_Duration..hours.)) +
  geom_point() +
  geom_smooth(method = "lm", col = "red") +
  labs(title = "Relación entre calorias quemadas y la duración de la sesion.",
       x = "Calorias quemadas.",
       y = "Duración de la sesion.") +
  annotate("text", x = 500, y = 2.0, label = paste("y =", round(coef(model)[1], 2), "+", round(coef(model)[2], 2), "x"), color = "red")
```

La linea nos muestra una relación alcista segun las calorias quemadas y el tiempo de entrenamiento. Que el valor de $\beta_0$ sea menor a 0 no nos importa mucho, debido a que el mínimo es 0.5 (media hora) por lo que nunca va a ser negarivo ya que $\beta_1$ es 721.79 

# Correlación

Vamos a calcular y entender la correlación entre las variables de nuestro estudio.

## Covarianza
```{r}
calorias <- datos$Calories_Burned
sesion <- datos$Session_Duration..hours.

covarianza <- round(cov(calorias, sesion), 3)

print(paste("La covarianza entre la calorias quemadas por la sesion de entrenamiento es: ", covarianza))
```

Con esta covarianza podemos observar que ambas variables aumentan juntas, además de que no es cercana a 0 por lo que entre estas variables hay una relación lineal entre las variables.

## Coeficiente de correlación de lineal.
Calculamos y mostramos el coeficiente de correlación de Pearson entre la duración de las sesiones de ejercicio y las calorías quemadas.

```{r}
correlacion_pearson <- cor(datos$Session_Duration..hours., datos$Calories_Burned, use = "complete.obs")
cat("El coeficiente de correlación de Pearson entre la duración de la sesión y las calorías quemadas es:", round(correlacion_pearson, 8), "\n")
```

La correlación de Pearson nos confirma lo que habiamos visto calculando la covarianza, como es cercano a 1, existe una relación lineal fuerte entre las variables y como es positivo, las variables aumentan a la par.

# Análisis de Residuos

Realizamos el análisis de residuos para evaluar el modelo ajustado.

### 1. Residuos vs Valores Ajustados

```{r}
# Obtener los residuos
residuos <- resid(modelo)
valores_ajustados <- fitted(modelo)

# Graficar residuos vs valores ajustados
plot(valores_ajustados, residuos,
     main = "Residuos vs Valores Ajustados",
     xlab = "Valores Ajustados",
     ylab = "Residuos",
     pch = 19, col = "blue")
abline(h = 0, col = "red", lwd = 2)

```

### 2. Histograma de los Residuos
```{r}
# Graficar histograma de los residuos
hist(residuos,
     main = "Histograma de Residuos",
     xlab = "Residuos",
     col = "lightblue", border = "black")

```




### 3. QQ-Plot de los Residuos


```{r}

# Graficar QQ-plot de los residuos
qqnorm(residuos, main = "QQ-Plot de los Residuos")
qqline(residuos, col = "red", lwd = 2) 

```

### 4. Prueba de Normalidad de los Residuos (Shapiro-Wilk)

```{r}
# Realizar la prueba de Shapiro-Wilk para normalidad de los residuos

shapiro_test <- shapiro.test(residuos)
cat("Prueba de Shapiro-Wilk para normalidad de los residuos:\n")
print(shapiro_test)

```

### 5. Residuos vs Tiempo de Estudio

```{r}
# Graficar residuos vs tiempo de estudio
plot(datos$Session_Duration..hours., residuos,
     main = "Residuos vs Tiempo de Estudio",
     xlab = "Duración de la Sesión (horas)",
     ylab = "Residuos",
     pch = 19, col = "purple")
abline(h = 0, col = "red", lwd = 2)

```


# Diagnóstico del Modelo: Pruebas de Homocedasticidad y Leverage

### 1. Prueba de Homocedasticidad (Breusch-Pagan)

```{r}
# Realizar la prueba de Breusch-Pagan para homocedasticidad
bptest_result <- bptest(modelo)
cat("\nPrueba de Breusch-Pagan para homocedasticidad:\n")
print(bptest_result)

```

### 2. Análisis de Leverage 

Calculamos el leverage de las observaciones y graficamos las observaciones con leverage alto.

```{r}
# Calcular leverage
leverage <- hatvalues(modelo)

# Umbral para leverage alto
n <- nrow(datos)
p <- length(coef(modelo))  # Número de parámetros (incluyendo el intercepto)
leverage_threshold <- 2 * p / n

# Identificar observaciones con leverage alto
leverage_high <- which(leverage > leverage_threshold)

cat("\nUmbral para leverage alto:", leverage_threshold, "\n")
cat("\nObservaciones con leverage alto (si las hay):\n")
print(leverage_high)

# Gráfico de leverage
grafico_leverage <- ggplot(data.frame(leverage), aes(x = seq_along(leverage), y = leverage)) +
  geom_point(color = "blue") +
  geom_hline(yintercept = leverage_threshold, col = "red", lwd = 2, lty = 2) +
  labs(title = "Leverage de las Observaciones", x = "Índice de Observación", y = "Leverage")

# Mostrar gráfico
grafico_leverage
```


### 3. Análisis de la Distancia de Cook

 mide la influencia de cada observación sobre los coeficientes del modelo.

```{r}
# Calcular Distancia de Cook
cooks_distance <- cooks.distance(modelo)

# Gráfico de Distancia de Cook
grafico_cooks_distance <- ggplot(data.frame(cooks_distance), aes(x = seq_along(cooks_distance), y = cooks_distance)) +
  geom_point(color = "blue") +
  geom_hline(yintercept = 4 / n, col = "red", lwd = 2, lty = 2) +
  labs(title = "Distancia de Cook", x = "Índice de Observación", y = "Distancia de Cook")

# Mostrar gráfico
grafico_cooks_distance

```


## Análisis de DFFITS

mide el cambio en los valores ajustados cuando se omite una observación.

```{r}
# Calcular DFFITS
dffits_values <- dffits(modelo)

# Gráfico de DFFITS
grafico_dffits <- ggplot(data.frame(dffits_values), aes(x = seq_along(dffits_values), y = dffits_values)) +
  geom_point(color = "green") +
  geom_hline(yintercept = c(2 * sqrt(length(coef(modelo)) / n)), col = "red", lwd = 2, lty = 2) +
  labs(title = "DFFITS", x = "Índice de Observación", y = "DFFITS")

# Mostrar gráfico
grafico_dffits

```


## Análisis de DFBETAS

nos indican la influencia de cada observación sobre cada coeficiente de la regresión.


```{r}
# Calcular DFBETAS# Calcular DFBETAS
dfbetas_values <- dfbetas(modelo)

# Convertir dfbetas_values a un data.frame para poder usarlo en ggplot
dfbetas_df <- as.data.frame(dfbetas_values)

# Graficar DFBETAS para el coeficiente de la pendiente (segunda columna si existe)
grafico_dfbetas <- ggplot(dfbetas_df, aes(x = seq_along(dfbetas_df[,2]), y = dfbetas_df[,2])) + # Usamos el coeficiente de la pendiente
  geom_point(color = "purple") +
  geom_hline(yintercept = 2 / sqrt(n), col = "red", lwd = 2, lty = 2) +
  labs(title = "DFBETAS para el Coeficiente de la Pendiente", x = "Índice de Observación", y = "DFBETAS")

# Mostrar gráfico
grafico_dfbetas


```




